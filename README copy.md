# C-QuAL – A Clinical Question Answering Benchmark for Long-Context Large Language Models

![Q-A-diagram (3)](https://github.com/user-attachments/assets/6f059669-6a81-47d6-829b-942d53e85008)

## Project Overview

The **C-QuAL** project focuses on leveraging large language models (LLMs) for assisting in clinical decision-making by automating question-answering (QA) tasks from electronic health records (EHRs). This project, developed as part of a team of MSc students, aims to create a more relevant and representative dataset for benchmarking the QA capabilities of LLMs, with the goal of deploying such models in clinical environments to improve decision reasoning and reduce clinician workload.

The project results in a novel clinical QA benchmarking dataset, C-QuAL, designed to evaluate the performance of LLMs, especially in long-context scenarios, by generating and annotating QA pairs from EHRs.

## Goals

- **Develop a Clinical QA Benchmark**: To create C-QuAL, a dataset that addresses the limitations of existing clinical QA benchmarks, making it more representative of real-world clinical scenarios.
- **Accelerate Clinical Decision Reasoning**: By assisting clinicians in navigating, retrieving, and summarizing data from EHRs, reducing time spent on manual tasks.
- **Benchmark LLMs**: Evaluate and benchmark the performance of different LLMs for QA tasks using the C-QuAL dataset.

## Dataset Overview

**C-QuAL** was developed using discharge summaries from the MIMIC-III dataset, which offers a more recent and representative clinical corpus compared to earlier datasets like n2c2. The framework for generating this dataset builds upon previous works like EHRNoteQA but improves upon them by leveraging long-context LLMs for processing multiple and longer discharge summaries.

### Dataset Features

- **Corpus**: The dataset is generated using the MIMIC-III database, focusing on patient discharge summaries to create QA pairs that cover various clinical aspects, such as treatments, diagnoses, and patient history.
- **Question Types**: Includes yes/no/maybe, temporal, factual, summarization, identification, and unanswerable questions, ensuring comprehensive evaluation of LLM capabilities.
- **Annotation**: GPT-4o. In future, human expert annotation will be used for improved dataset quality.

## Folder Structure

The project includes two primary components: **generation** and **evaluation**.

```
.
├── analysis
├── annotations
│   └── checkpoints
├── benchmarking-results
├── generations
│   └── checkpoints
├── model-answers
└── processing
```

- **`generation/`**: Contains the `generate.py` script, which allows users to specify model configurations and the number of discharge summaries used to generate QA pairs.
- **`evals/`**: Includes scripts for evaluating the generated QA pairs and benchmarking LLM performance locally or on Microsoft Azure.

## Setup Instructions

1. **Prerequisites**:

   - Python 3.7+
   - A Microsoft Azure Cloud subscription with access to OpenAI-based LLM services (due to HIPAA agreement for processing MIMIC-III data).

2. **Install Dependencies**:
   Run the following command to install necessary packages:

   ```bash
   pip install -r requirements.txt
   ```

3. **Generate Dataset**:
   Use the `generate.py` script to generate QA pairs by specifying model configurations:

   ```bash
   python generation/generate.py
   ```

4. **Evaluation**:
   Run the evaluation scripts to benchmark model performance:
   ```bash
   python evals/[file_name].py
   ```

## Usage of C-QuAL

The **C-QuAL** dataset is designed for evaluating LLMs on clinical QA tasks. It focuses on ensuring that the models are tested on realistic, complex clinical questions. The dataset can be used to:

- Benchmark the performance of long-context LLMs on clinical QA tasks.
- Assist in the selection of the most suitable LLM for clinical deployment by evaluating their ability to reason, summarise, and retrieve information from EHRs.

### Output

The generated datasets are stored as CSV files in the `data/` directory, structured as follows:

- **Discharge Summary**: The clinical text provided to the model.
- **Question**: The question posed based on the summary.
- **Correct Answer**: The annotated answer.
- **Model Answer**: The answer generated by the model.

## Contributions

The project was developed under the guidance of **Professor Yulan He** and with insights from clinicians to ensure clinical relevance. The dataset and generation framework are open-source and hosted on GitHub under the MIT license, encouraging further research and improvements in the field of clinical QA using LLMs.
